{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f50ODjhO9CSZ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W7e0w-139Iij"
   },
   "source": [
    "### 1. Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SSsmLzjE9s-a"
   },
   "source": [
    "\n",
    "Let's start by writing some functions for activation functions that we would like to be able to use.\n",
    "\n",
    "Fill in the functions below to implement the associated activation functions. Any time you need a special function (e.g. exponentation), try to find a version in NumPy so that your activation functions will work on single values as well as arrays.\n",
    "\n",
    "*bonus*: try to implment the ReLU activation function so that it works elementwise on a NumPy -- this is called \"vectorizing\" your code. Hint: check out the `np.where` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yr4mkpLh9yGP"
   },
   "outputs": [],
   "source": [
    "def linear(z):\n",
    "  '''\n",
    "  linear activation function\n",
    "  '''\n",
    "  return z\n",
    "\n",
    "# more specifically, the logistic sigmoid that has values between 0 and 1\n",
    "def sigmoid(z):\n",
    "  '''\n",
    "  sigmoid activation function\n",
    "  '''\n",
    "  return 1 / (1 + np.exp(-z))\n",
    "  \n",
    "\n",
    "def tanh(z):\n",
    "  '''\n",
    "  tanh activation function\n",
    "  '''\n",
    "  return np.tanh(z)\n",
    "\n",
    "def relu(z):\n",
    "  if z < 0:\n",
    "    return 0 \n",
    "  else: \n",
    "    return z\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-VEZ_W1V7yUv"
   },
   "outputs": [],
   "source": [
    "a = np.array([-1.0, 0.0, 1.0])\n",
    "\n",
    "np.testing.assert_equal(linear(5), 5)\n",
    "np.testing.assert_equal(linear(-3.0), -3.0)\n",
    "np.testing.assert_array_equal(linear(a), a)\n",
    "\n",
    "np.testing.assert_equal(sigmoid(0.0), 0.5)\n",
    "np.testing.assert_allclose(sigmoid(a), [0.26894142, 0.5, 0.73105858])\n",
    "\n",
    "np.testing.assert_equal(tanh(0.0), 0.0)\n",
    "np.testing.assert_allclose(tanh(a), [-0.76159416, 0.0, 0.76159416])\n",
    "\n",
    "np.testing.assert_equal(relu(5), 5)\n",
    "np.testing.assert_equal(relu(-5), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M3R7FZ8y9S1v"
   },
   "source": [
    "### 2. Try it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "edvxHceU_XkJ"
   },
   "source": [
    "Let's reuse our neural net layer function from last time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IbMrGPe9_X2d"
   },
   "outputs": [],
   "source": [
    "def nn_layer(X, W, b, f):\n",
    "  return f(np.dot(X, W) + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XLIsi2EuAkut"
   },
   "outputs": [],
   "source": [
    "def nn_two_layers(X, W_1, b_1, f_1, W_2, b_2, f_2):\n",
    "  H = nn_layer(X, W_1, b_1, f_1)\n",
    "  Y_hat = nn_layer(H, W_2, b_2, f_2)\n",
    "  return Y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sf7oiWBp_jbE"
   },
   "source": [
    "But now, we have multiple activation functions to try out. As we did previously, create randomized weight matrices for a network with scalar input, scalar output and any number of hidden nodes in a single layer. Generate plots of this input output relationships.\n",
    "\n",
    "This time, try out different activation functions: linear, sigmoid, tanh, relu and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Er92ByXX_egf"
   },
   "outputs": [],
   "source": [
    "# Set the input dimension, the number of hidden units, and the number of ouptput units\n",
    "n_input, n_hidden, n_output = 1, 15, 1\n",
    "\n",
    "# We want get the NN's output for a range of input values, so that we cant plot\n",
    "# input vs output. We can get evenly space values using `np.linspace`. We also\n",
    "# want to process these inputs as a \"batch\", so we use `np.newaxis` to turn this\n",
    "# 1-d array into a 2-d array with a single column.\n",
    "n_grid = 100\n",
    "X = np.linspace(-10, 10, n_grid)[:, np.newaxis]\n",
    "\n",
    "# We can generate random values (drawn from a standard gaussian distribution --\n",
    "# mean = 0, standard deviation = 1), with `np.random.randn(shape)`\n",
    "W_1 = np.random.randn(n_input, n_hidden)\n",
    "b_1 = np.random.randn(n_hidden)\n",
    "W_2 = np.random.randn(n_hidden, n_output)\n",
    "b_2 = np.random.randn(n_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LBI23a_F9Xuu"
   },
   "source": [
    "### 3.  Outputs and Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kgPrHIxbBEVy"
   },
   "source": [
    "Next, we'll define some common Output activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dQxHU6t3PraV"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "  '''\n",
    "  sigmoid output function\n",
    "  '''\n",
    "  return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def softmax(z):\n",
    "  '''\n",
    "  softmax output function\n",
    "  '''\n",
    "  row_sums = (np.exp(z).sum(axis = 1)) [:, np.newaxis]\n",
    "  return np.exp(z) / row_sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AAFko6cqRFP6"
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(\n",
    "    sigmoid(2),\n",
    "    np.array([.88]),\n",
    "    decimal=2\n",
    ")\n",
    "\n",
    "np.testing.assert_almost_equal(\n",
    "    sigmoid(np.array([2,-2,1,-1])),\n",
    "    np.array([.88, .12, .73, .27]),\n",
    "    decimal=2\n",
    ")\n",
    "\n",
    "np.testing.assert_almost_equal(\n",
    "    softmax(np.array([[2,5,1], [3,1,1]])),\n",
    "    np.array([[0.046, 0.93, 0.017],\n",
    "       [0.78, 0.10, 0.10]]),\n",
    "    decimal=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xNrgYJtqBKyw"
   },
   "source": [
    "Next, we'll define some common Loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PMvoYMgSBNiD"
   },
   "outputs": [],
   "source": [
    "def squared_error(y_hat, y_true):\n",
    "  '''\n",
    "  squared error loss\n",
    "  '''\n",
    "  return ((y_hat - y_true)**2)\n",
    "\n",
    "\n",
    "def binary_crossentropy(y_hat, y_true):\n",
    "  '''\n",
    "  binary crossentropy loss for label-encoded inputs\n",
    "  '''\n",
    "  return - y_true*np.log(y_hat) - (1-y_true)*np.log(1 - y_hat)\n",
    "\n",
    "\n",
    "def binary_crossentropy_onehot(y_hat, y_true):\n",
    "  '''\n",
    "  binary crossentropy loss for onehot-encoded inputs\n",
    "  '''\n",
    "  return - (y_true * np.log(y_hat)).sum(axis=1)\n",
    "\n",
    "\n",
    "def categorical_crossentropy(y_hat, y_true):\n",
    "  '''\n",
    "  categorical crossentropy loss for onehot-encoded inputs\n",
    "  '''\n",
    "  return - (y_true * np.log(y_hat)).sum(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LgAP3aFeEWOX"
   },
   "outputs": [],
   "source": [
    "np.testing.assert_equal(\n",
    "    squared_error(np.array([1,2,4]), np.array([5,4,3])),\n",
    "    np.array([16, 4, 1])\n",
    ")\n",
    "\n",
    "\n",
    "np.testing.assert_almost_equal(\n",
    "    binary_crossentropy(\n",
    "        np.array([.51, .49, .99, 0.01, .99 ]), \n",
    "        np.array([1, 1, 1, 1, 0])),\n",
    "    np.array([0.67, 0.71, 0.01, 4.60, 4.60]),\n",
    "    decimal=2\n",
    ")\n",
    "\n",
    "np.testing.assert_almost_equal(\n",
    "    binary_crossentropy_onehot(\n",
    "        np.array([[.49, .51], [.51, .49], [.01, .99], [0.99, .01], [.01, .99] ]), \n",
    "        np.array([[0, 1], [0, 1], [0, 1], [0, 1], [1, 0]])),\n",
    "    np.array([0.67, 0.71, 0.01, 4.60, 4.60]),\n",
    "    decimal=2\n",
    ")\n",
    "\n",
    "np.testing.assert_almost_equal(\n",
    "      categorical_crossentropy(\n",
    "        np.array([[.4, .5, .1], [.2, .2, .6]  ]), \n",
    "        np.array([[0, 1, 0], [0, 1, 0],])),\n",
    "    np.array([0.69, 1.6]),\n",
    "    decimal=2\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8FZ8gj-9dkd"
   },
   "source": [
    "###4. Discussion problem with your groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cCdSFuKmTk7v"
   },
   "source": [
    "Suppose we want to predict if a person is a credit risk (Yes or No) based on their {Income, Age, YearsOfEducation}. Draw a diagram of a possible neural network (with one hidden layer) to fit a datset like this. \n",
    "* How many input nodes and output nodes are there?\n",
    "* Pick how many nodes are in the hidden layer. Pick an activation function in the hidden layer. Include bias nodes at the hidden layer and the output layer.\n",
    "* What activation function would you pick for the output layer?\n",
    "* What loss function would you pick for fitting this model?\n",
    "* How many total free parameters are in this network? \n",
    "* Write an equation for the output of this network as a function of its input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "328cA6hiUEFO"
   },
   "source": [
    "*   This is a binary classification problem. Thus, there are 3 input nodes and 1 output nodes, where if the probability is larger than the threshold (for instance, 0.5), than it's a \"yes\" and vice versa. \n",
    "*   The network would have one hidden layer with 2 nodes, since the optimal size of the hidden layer is usually between the size of the input and size of the output layers. We could try to use ReLu as the hidden layer activation function, since it doesn't saturate with very small or large values and Income, Age, YearsOfEducation are all supposed to be positive values. \n",
    "*   The activation function for the output layer should be a sigmoid function.\n",
    "*   Binary cross entropy loss function is picked for fitting this model. \n",
    "*   There are 3 bias terms and 8 weights. Thus, there are 11 free parameters. \n",
    "*   See the following image for the equation\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab2_ffnn_part2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
